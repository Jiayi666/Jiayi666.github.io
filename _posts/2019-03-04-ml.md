---
layout:     post
title:      "Machine Learning Interview Preparation"
subtitle:   "ML related"
date:       2019-03-04
author:     "Jiayi.Liu"
header-img: "img/post-bg-2015.jpg"
catalog: 	true
tags:
    - Machine Learning
    - Data Science
---

> Firstly think about what is the problem we are analyzing. Regression? Classification? Unsupervised?

> For all ML problems, there are always some important entities: **data, model, objective function**. Our technology like p-vaule is for data (null hypothesis), multiple model and loss is for the rest. Take a look at [this answer](https://qr.ae/TW7ivG), the writer really mastered such idea for analyzing DS problems.

> Remember that Ridge and Lasso *don't regularize bias term!* Check [here](https://stats.stackexchange.com/questions/86991/reason-for-not-shrinking-the-bias-intercept-term-in-regression/161689) for more detail.

### To Do

* Python implementation.
* SQL.

### Precision & Recall (Sensitivity)

> Precision is "**how useful the search results are**", and recall is "**how complete the results are**".

&nbsp;&nbsp;&nbsp;&nbsp;Suppose a computer program for recognizing dogs in photographs identifies 8 dogs in a picture containing 12 dogs and some cats. Of the 8 identified as dogs, 5 actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12. When a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3.

#### AUC and ROC

> For more details than below, check [this tutorial](https://medium.com/greyatom/lets-learn-about-auc-roc-curve-4a94b4d88152).

> True positive rate = recall = sensitivity

> False positive rate = fall out = 1 - specificity
 
> Specificity = True negative rate

> AUC is the area under the **ROC** curve!

&nbsp;&nbsp;&nbsp;&nbsp;AUC (area under curve) is evaluating the rate for **classify positive to positive and negative to negative**. So, in perfect situation, we should get `AUC=1`.

> A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a **binary classifier** system **as its discrimination threshold is varied**.

&nbsp;&nbsp;&nbsp;&nbsp;ROC is a curve with y axis to be **true positive rate (TPR)** and x axis to be **false positive rate (FPR)**. Actually the FPR is just `1 - True Negative Rate` (Specificity, compared with sensitivity/recall). Sensitivity represent the model can correctly classify positive to positive and specificity says the model can classify negative to negative.

&nbsp;&nbsp;&nbsp;&nbsp;For how to translate the above relationship into a curve, [this tutorial](https://medium.com/greyatom/lets-learn-about-auc-roc-curve-4a94b4d88152) described this relationship as **shifting the threshold**, which is very impressive.

### F1 Score

> Harmonic average of recall & precision.

&nbsp;&nbsp;&nbsp;&nbsp;The F1 score is an evaluation method for **binary classification**. In short, the F1 score is just a *harmonic mean of precision and recall*. The harmonic mean for `n` numbers can be find [here](https://en.wikipedia.org/wiki/Harmonic_mean). So, F1 score is defined as [this](https://wikimedia.org/api/rest_v1/media/math/render/svg/057ffc6b4fa80dc1c0e1f2f1f6b598c38cdd7c23).

&nbsp;&nbsp;&nbsp;&nbsp;In the best situation, the classification has perfect precision and recall, `F1=1` while in the worst case `F1=0`.

### Generative & Discrimitive Model

&nbsp;&nbsp;&nbsp;&nbsp;As shown [here](https://medium.com/@mlengineer/generative-and-discriminative-models-af5637a66a3), discrimitive model is trying to the **difference between two classes** `P(Y|X)` while generative model is trying to learn each class itself `P(X,Y)`.

### Logistic Regression

> **Linear model** using **sigmoid function** to squash output and used **cross-entropy loss**.

&nbsp;&nbsp;&nbsp;&nbsp;The vanilla logistic regression is designed for binary classification. According to [this tutorial](https://towardsdatascience.com/building-a-logistic-regression-in-python-301d27367c24), linear model, sigmoid function and cross-entropy loss is the main element for logistic regression.

&nbsp;&nbsp;&nbsp;&nbsp;To use logistic regression in multinomial/multi-label situation, we can either use `one versus all` schema for multiple times or change the sigmoid function to **softmax**. The multiclass cross entropy loss is just **the sum of binary cross entropy for all classes**, check it in [here](https://datascience.stackexchange.com/questions/9302/the-cross-entropy-error-function-in-neural-networks).

&nbsp;&nbsp;&nbsp;&nbsp;The `sklearn` implementation is simple, just see [here](https://towardsdatascience.com/building-a-logistic-regression-in-python-301d27367c24). Some key points:

1. `from sklearn.linear_model import LogisticRegression`
2. `from sklearn.metrics improt accuracy_score`
3. `rand = np.random.rand((10,10))`
4. `model.fit(x,y)`
5. `model.predict(x)`
6. `acc = accuracy_score(y,predicted_y)`

### A/B Test

&nbsp;&nbsp;&nbsp;&nbsp;No more than **控制变量法**, the key points is **randomly assign customer** and **only one variable**, then we can have clear causal relationship between the changed variable and the change of metric (result).

### P-value and null hypothesis test

> For what is p-value and null hypothesis, check [this tutorial](https://www.spss-tutorials.com/null-hypothesis/).

> How to calculate p-value, chech [this tutorial](https://www.wikihow.com/Calculate-P-Value).

> A great example of using p-value, chech [here, the first comment](https://towardsdatascience.com/my-take-on-google-ai-interview-question-with-interactive-code-part-1-db2e33a26f10).

&nbsp;&nbsp;&nbsp;&nbsp;P-value is to **test is the null hypothesis hold or not.** 

&nbsp;&nbsp;&nbsp;&nbsp;The null hypothesis is normally our **guess** for the system, for example the chance for getting head and tail are both 0.5 if your toss a coin (in [this tutorial](https://www.spss-tutorials.com/null-hypothesis/), this kind of null hypothesis is called 'point estimate'). But if we toss a coin 10 times and get 6 head, **does that mean the coin is biased (the null hypothesis is wrong) or this is caused by chance (variance)?** A p-value will help us determine this! For how to calculate p-value, check the tutorial I linked above. But really calculating a p-value includes the process of **checking in table** as shown in the above tutorial, we need to know more about the idea behind it.

&nbsp;&nbsp;&nbsp;&nbsp;Significance levels are written as a decimal (such as 0.01), which corresponds to the percent chance that random sampling would produce a difference as large as the one you observed if there was no underlying difference in the populations. 简单来说 significance level 决定了 **how sure you want** about your judgement (about null hypothesis).

### Boosting & Bagging

> Boosting and Bagging are both to **reduce overfitting** with bootstraping (resample with put back). [This tutorial](https://hackernoon.com/how-to-develop-a-robust-algorithm-c38e08f32201) explained perfectly for both bagging and boosting.

&nbsp;&nbsp;&nbsp;&nbsp;For bagging, each model is trained on a subset of the training data and the final result is achieved with averaging or voting.

&nbsp;&nbsp;&nbsp;&nbsp;For boosting, the N models are trained **sequentially** as shown in [this tutorial](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/). In boosting, the *selection of data* is determined by previous model. The data point that misclassified by previous models are more likely to be chosen. This idea is the same for taking the most informative action in RL.

### SVM 

> Just linear model with hinge loss. Logistic regression is linear model with sigmoid function to squash the output.

&nbsp;&nbsp;&nbsp;&nbsp;SVM is not only trying to find the hyperplane that separate nodes, but the hyperplane **with largest margin**. According to [this tutorial](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47), **support vectors** are **data points** that are closer to the hyperplane and influence the position and orientation of the hyperplane.

&nbsp;&nbsp;&nbsp;&nbsp;To find the max margin, **hinge loss is used**, `loss=max(0,1-y*f(x))`.

&nbsp;&nbsp;&nbsp;&nbsp;Sklearn implementation:

1. Test train split & KFold-cross validation: `from sklearn.model_selection import train_test_split, KFold `
2. `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20) `
3. `from sklearn.svm import SVC`
4. `svclassifier = SVC(kernel='linear')`
5. fit + predict
6. `from sklearn.metrics import confusion_matrix, accuracy_score` 

### Random Forest

> Decision tree + Bagging!

&nbsp;&nbsp;&nbsp;&nbsp;Random Forest adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model.

### Matplotlib

* `plt.plot(x,y,label="")` is to draw curves.
* `plt.scatter(x,y,label="")` is to draw scatters.

### Regression Analysis

&nbsp;&nbsp;&nbsp;&nbsp;"Regression residual analysis" is used to check if a regression model is proper for the data or not. Check [here](https://stattrek.com/regression/residual-analysis.aspx) for more information.

### Type 1 & Type 2 Error

- Type 1: false positive, tell a healthy man he has disease.
- Type 2: false negative, tell an ill man he is ok.

&nbsp;&nbsp;&nbsp;&nbsp;Remember that H0 (null hypothesis) is normally assuming **not happending**, so Type 1 error rejected H0 and Type 2 error accepted H0 by mistake. Check more [here](https://www.springboard.com/blog/machine-learning-interview-questions/).

### Adam optimizer

> Check more [here](http://ruder.io/optimizing-gradient-descent/index.html#nesterovacceleratedgradient).

&nbsp;&nbsp;&nbsp;&nbsp;Adam is a combination of *momentum* and *RSMprop*.

- Momentum: use the trend from previous gradient to *correct* current gradient.
- RSMprop: automatically change **learning rate** based on the expectation of gradient.

### On policy & Off policy update

> Q-learning is off policy alg while SARSA is on policy alg!

- On policy: updating not related with (actual) action. In q-learning, we can update as long as we have current q-value.
- Off policy: update based on what action is taking. In sarsa, we use the actually choosed action to update.

### Q-Q plot

> Normal Quantile-Quantile plot: used to determine is a set of observation is **normally distributed**.

> If the observation is normally distributed, the Q-Q plot should looks like a straight line.

&nbsp;&nbsp;&nbsp;&nbsp;Given a set of `n` observation `O`, the following procedure give the QQ plot:

1. Sort `O` in increasing order.
2. Divide the standard normal distribution into `n+1` part with equal probability (there will be `n` spliting point).
3. Use sorted `O` as y values, the value at **spliting point** as x values to draw a **scatter plot** which is the QQ plot.

### Principal Component Analysis (PCA)

> Take care that using PCA, the new feature will be **harder to interpret**!

> The basic idea for PCA: pick **directions/eigon vector** that has larger eigon value. Check [here](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c) for more details.

- Covariance Matrix for `X`: 
	1. Subtract each column with mean `Z=X-mean_column(X)`
	2. Normalize each column with variance `Z=Z/variance_column(Z)`
	3. Covariance matrix is `Z^TZ`.
- Single Value Decomposition (SVD):
	1. `Z^TZ = PDP^(-1)` where `P` is the matrix for eigon vector and D is a diagonal matrix with eigon value.

&nbsp;&nbsp;&nbsp;&nbsp;Remeber the output of PCA is a **new dataset!** Here we are going to use `Z*=ZP*` where `P*` is the matrix may only contains a part of eigon vectors.

### Gausian Mixed Model & Expectation Maximization (EM)

> [Here](https://www.youtube.com/watch?v=qMTuMa86NzU) is a perfect explaination for GMM and EM! Must see!

> Gausian Mixed Model is just a summation of multiple Gausian models.

> "Expectation" at here means **likelihood**!

#### Probability vs Likelihood

- Probability: for a model, **given parameters** the possibility for an **observation**.
- Likelihood: **given observation**, the possibility for parameters.

#### GMM & EM

> K-means can only work with circle distribution based on its **centroid** method. GMM can be used for ellipse.

> K-means can be treated as a special case for GMM where the covariance matrix is a scalar times an identity matrix.

&nbsp;&nbsp;&nbsp;&nbsp;The work for EM is to **estimate parameters that maximize likelihood for the dataset.** In a GMM, the parameters are `mu`,`delta`, and weight for each Gaussian model.

1. Given dataset `X` with `n` data point.
2. Initialize `mu`, `delta`, and `pi` where `pi` is the weight for each Guassian model.
3. Estimate (E-step): calculate the responsibility **for each Guassian model in each data point**, get a `N*K` matrix where `K` is the number of Guassian model used. The responsibility is defined as *chance for this data point to be **generated by** this model divided by the sum of the data point is generated by each model, all weighted by `pi`*.
4. Maximize (M-step): Update `mu`, `delta`, `pi` based on its responsibilies. It easier to understand this process with *matrix operation*, check the tutorial at the beginning of this section for more detail.

