---
layout:     post
title:      "Machine Learning Interview Preparation"
subtitle:   "ML related"
date:       2019-03-04
author:     "Jiayi.Liu"
header-img: "img/post-bg-2015.jpg"
catalog: 	true
tags:
    - LeetCode
    - Algorithm
    - Data Structure
---

> Firstly think about what is the problem we are analyzing. Regression? Classification? Unsupervised?

> For all ML problems, there are always some important entities: **data, model, objective function**. Our technology like p-vaule is for data (null hypothesis), multiple model and loss is for the rest.

### To Do

* Calculate the monkey bayes problem.
* SVM.
* Random Forest.

### Precision & Recall

> Precision is "**how useful the search results are**", and recall is "**how complete the results are**".

&nbsp;&nbsp;&nbsp;&nbsp;Suppose a computer program for recognizing dogs in photographs identifies 8 dogs in a picture containing 12 dogs and some cats. Of the 8 identified as dogs, 5 actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12. When a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3.

### F1 Score

&nbsp;&nbsp;&nbsp;&nbsp;The F1 score is an evaluation method for **binary classification**. In short, the F1 score is just a *harmonic mean of precision and recall*. The harmonic mean for `n` numbers can be find [here](https://en.wikipedia.org/wiki/Harmonic_mean). So, F1 score is defined as [this](https://wikimedia.org/api/rest_v1/media/math/render/svg/057ffc6b4fa80dc1c0e1f2f1f6b598c38cdd7c23).

&nbsp;&nbsp;&nbsp;&nbsp;In the best situation, the classification has perfect precision and recall, `F1=1` while in the worst case `F1=0`.

### Generative & Discrimitive Model

&nbsp;&nbsp;&nbsp;&nbsp;As shown [here](https://medium.com/@mlengineer/generative-and-discriminative-models-af5637a66a3), discrimitive model is trying to the **difference between two classes** `P(Y|X)` while generative model is trying to learn each class itself `P(X,Y)`.

### Logistic Regression

> **Linear model** using **sigmoid function** to squash output and used **cross-entropy loss**.

&nbsp;&nbsp;&nbsp;&nbsp;The vanilla logistic regression is designed for binary classification. According to [this tutorial](https://towardsdatascience.com/building-a-logistic-regression-in-python-301d27367c24), linear model, sigmoid function and cross-entropy loss is the main element for logistic regression.

&nbsp;&nbsp;&nbsp;&nbsp;To use logistic regression in multinomial/multi-label situation, we can either use `one versus all` schema for multiple times or change the loss function to **cross entropy loss**.

### A/B Test

&nbsp;&nbsp;&nbsp;&nbsp;No more than **控制变量法**, the key points is **randomly assign customer** and **only one variable**, then we can have clear causal relationship between the changed variable and the change of metric (result).

### P-value and null hypothesis test

> How to calculate p-value, chech [this tutorial](https://www.wikihow.com/Calculate-P-Value).

> A great example of using p-value, chech [here, the first comment](https://towardsdatascience.com/my-take-on-google-ai-interview-question-with-interactive-code-part-1-db2e33a26f10).

&nbsp;&nbsp;&nbsp;&nbsp;P-value is to **test is the null hypothesis hold or not.** 

&nbsp;&nbsp;&nbsp;&nbsp;The null hypothesis is normally our **guess** for the system, for example the chance for getting head and tail are both 0.5 if your toss a coin. But if we toss a coin 10 times and get 6 head, **does that mean the coin is biased (the null hypothesis is wrong) or this is caused by chance (variance)?** A p-value will help us determine this! For how to calculate p-value, check the tutorial I linked above. But really calculating a p-value includes the process of **checking in table** as shown in the above tutorial, we need to know more about the idea behind it.

&nbsp;&nbsp;&nbsp;&nbsp;Significance levels are written as a decimal (such as 0.01), which corresponds to the percent chance that random sampling would produce a difference as large as the one you observed if there was no underlying difference in the populations. 简单来说 significance level 决定了 **how sure you want** about your judgement (about null hypothesis).

### Boosting & Bagging

> Boosting and Bagging are both to **reduce overfitting** with bootstraping (resample with put back). [This tutorial](https://hackernoon.com/how-to-develop-a-robust-algorithm-c38e08f32201) explained perfectly for both bagging and boosting.

&nbsp;&nbsp;&nbsp;&nbsp;For bagging, each model is trained on a subset of the training data and the final result is achieved with averaging or voting.

&nbsp;&nbsp;&nbsp;&nbsp;For boosting, the N models are trained **sequentially** as shown in [this tutorial](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/). In boosting, the *selection of data* is determined by previous model. The data point that misclassified by previous models are more likely to be chosen. This idea is the same for taking the most informative action in RL.

### SVM 

> Just linear model with hinge loss. Logistic regression is linear model with sigmoid function to squash the output.

&nbsp;&nbsp;&nbsp;&nbsp;SVM is not only trying to find the hyperplane that separate nodes, but the hyperplane **with largest margin**. According to [this tutorial](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47), **support vectors** are **data points** that are closer to the hyperplane and influence the position and orientation of the hyperplane.

&nbsp;&nbsp;&nbsp;&nbsp;To find the max margin, **hinge loss is used**.